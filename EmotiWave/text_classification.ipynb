{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1NcdE6eWBCC6bovRMR8NutU53vvy9xxCD",
      "authorship_tag": "ABX9TyPzCvjVa9rNv5UliN0J26Rl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visionbyangelic/EmotiWave/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#go emotion preprocessing\n"
      ],
      "metadata": {
        "id": "BqXcwZQy5CVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zC6kXy73D2k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neattext\n"
      ],
      "metadata": {
        "id": "F2GsKRzgD51Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV3gH-4b4eli"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import re\n",
        "import neattext.functions as nfx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n"
      ],
      "metadata": {
        "id": "WxkRk9Rn4uSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU availability for future steps\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "DdUFyMIO5n3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emotion_dataset_raw.csv\")\n",
        "print('dataset preview:')\n",
        "print(df.head())\n",
        "print('dataset shape:')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "z3rPHbtx6pSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove shame and disgust\n",
        "df = df[df['Emotion'] != 'disgust']\n",
        "df = df[df['Emotion'] != 'shame']"
      ],
      "metadata": {
        "id": "Mh7DLWNL7fYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter emotions to match fer2013\n",
        "emotion_map = {\n",
        "    'anger': 'anger',\n",
        "    'fear': 'fear',\n",
        "    'joy': 'happy',\n",
        "    'sadness': 'sad',\n",
        "    'surprise': 'surprise',\n",
        "    'neutral': 'neutral'\n",
        "}\n",
        "df = df[df['Emotion'].isin(emotion_map.keys())]\n",
        "df['Emotion'] = df['Emotion'].map(emotion_map)\n",
        "print(\"\\nEmotions after filtering:\", df['Emotion'].unique())"
      ],
      "metadata": {
        "id": "JRMvIHpH7L0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean text\n",
        "!pip install neattext\n",
        "\n",
        "import neattext.functions as nfx\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['Clean_Text'] = df['Text'].apply(nfx.remove_userhandles).apply(clean_text)\n",
        "print(\"\\nSample cleaned text:\")\n",
        "print(df['Clean_Text'].head())"
      ],
      "metadata": {
        "id": "PY27DFMp7rRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check data integrity\n",
        "print(\"\\nRows with missing text:\", df['Clean_Text'].isnull().sum())\n",
        "print(\"Rows with missing emotion labels:\", df['Emotion'].isnull().sum())\n",
        "print(\"Rows with text length < 1:\", (df['Clean_Text'].str.len() < 1).sum())\n",
        "df = df.dropna(subset=['Clean_Text', 'Emotion'])\n",
        "df = df[df['Clean_Text'].str.len() >= 1]\n"
      ],
      "metadata": {
        "id": "6djqVkbD8p6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize emotion distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "order = df['Emotion'].value_counts().index\n",
        "sns.countplot(x='Emotion', data=df, order=order)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Emotion Class Distribution (Full Dataset)\")\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_Q0UsQEy872X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into train (80%), validation (10%), test (10%)\n",
        "X = df['Clean_Text']\n",
        "y = df['Emotion']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "UMIpJOy69Enf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes\n",
        "train_df = pd.DataFrame({'Text': X_train, 'Emotion': y_train}).reset_index(drop=True)\n",
        "val_df = pd.DataFrame({'Text': X_val, 'Emotion': y_val}).reset_index(drop=True)\n",
        "test_df = pd.DataFrame({'Text': X_test, 'Emotion': y_test}).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "J4Vn2agV9J72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "print(df['Emotion'].unique())\n",
        "print(df['Emotion'].value_counts())"
      ],
      "metadata": {
        "id": "0srw0gf_9NFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Balance training set (undersample 'happy' to 6,720)\n",
        "emotion_counts = train_df['Emotion'].value_counts()\n",
        "print(\"\\nOriginal training emotion distribution:\\n\", emotion_counts)\n",
        "if emotion_counts.get('happy', 0) > 6720:\n",
        "    happy_df = train_df[train_df['Emotion'] == 'happy'].sample(n=6720, random_state=42)\n",
        "    other_df = train_df[train_df['Emotion'] != 'happy']\n",
        "    train_df = pd.concat([happy_df, other_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"\\nBalanced training emotion distribution:\\n\", train_df['Emotion'].value_counts())\n"
      ],
      "metadata": {
        "id": "BXKLCpAj-oZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neutral dropped which could harm the model performance\n",
        "\n"
      ],
      "metadata": {
        "id": "7OLoonLN-uL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot balanced training distribution\n",
        "order = df['Emotion'].value_counts().index\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Emotion', data=train_df, order=order)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Balanced Training Emotion Distribution\")\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jOXl5H16DOf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Map emotions to numeric labels\n",
        "label_map = {\n",
        "    'anger': 0,\n",
        "    'fear': 1,\n",
        "    'happy': 2,\n",
        "    'sad': 3,\n",
        "    'surprise': 4,\n",
        "    'neutral': 5\n",
        "}\n",
        "train_df['Emotion'] = train_df['Emotion'].map(label_map)\n",
        "val_df['Emotion'] = val_df['Emotion'].map(label_map)\n",
        "test_df['Emotion'] = test_df['Emotion'].map(label_map)\n",
        "print(\"\\nUnique emotion labels after mapping:\", sorted(train_df['Emotion'].unique()))\n"
      ],
      "metadata": {
        "id": "m3DQpSmPD-H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save preprocessed data\n",
        "train_df.to_csv('/content/goemotions_train.csv', index=False)\n",
        "val_df.to_csv('/content/goemotions_val.csv', index=False)\n",
        "test_df.to_csv('/content/goemotions_test.csv', index=False)\n",
        "\n",
        "# Confirm split sizes\n",
        "print(f\"\\nTraining samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "# Download CSVs\n",
        "from google.colab import files\n",
        "files.download('/content/goemotions_train.csv')\n",
        "files.download('/content/goemotions_val.csv')\n",
        "files.download('/content/goemotions_test.csv')"
      ],
      "metadata": {
        "id": "wV4nC5_BD_7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save CSVs to Google Drive\n",
        "train_df.to_csv('/content/drive/MyDrive/goemotions_train.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/goemotions_val.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/goemotions_test.csv', index=False)"
      ],
      "metadata": {
        "id": "PK8Ls6QnNw8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training"
      ],
      "metadata": {
        "id": "hwefuD04JfED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using focal loss and class weight to habdle the imbalance especially in neutral in comparison to happy.\n",
        "\n",
        "also using pytorch dataset and dataloader which aims to oprepare the preprocessed data for bert based text classification miroring fer2013"
      ],
      "metadata": {
        "id": "BJpZDv5XJjku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C6UXC2ZCDVeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import os"
      ],
      "metadata": {
        "id": "zxysK8E0LH7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set random seed for reproductibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Verify GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "RJsvq0gkLnja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preprocessed data\n",
        "train_df = pd.read_csv('/content/goemotions_train.csv')\n",
        "val_df = pd.read_csv('/content/goemotions_val.csv')\n",
        "test_df = pd.read_csv('/content/goemotions_test.csv')\n"
      ],
      "metadata": {
        "id": "S2borxQTLy4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify training distribution for class weights\n",
        "print(\"Training emotion distribution:\")\n",
        "print(train_df['Emotion'].value_counts())"
      ],
      "metadata": {
        "id": "qqBRCQC5OZbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom Dataset\n",
        "class GoEmotionsDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.df['Text'].iloc[idx])\n",
        "        label = int(self.df['Emotion'].iloc[idx])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "pYR4sK7bOahj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "aQVygCmnOho7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = GoEmotionsDataset(train_df, tokenizer, max_length=128)\n",
        "val_dataset = GoEmotionsDataset(val_df, tokenizer, max_length=128)\n",
        "test_dataset = GoEmotionsDataset(test_df, tokenizer, max_length=128)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32  # For Tesla T4\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "V6yPqV8DOihz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights for focal loss\n",
        "class_counts = train_df['Emotion'].value_counts().sort_index()  # [anger, fear, happy, sad, surprise, neutral]\n",
        "total_samples = len(train_df)\n",
        "class_weights = torch.tensor([total_samples / (6 * count) for count in class_counts], dtype=torch.float).to(device)\n",
        "print(\"\\nClass weights for focal loss:\", class_weights)\n",
        "\n",
        "# Verify DataLoader\n",
        "def verify_dataloader(loader, name):\n",
        "    print(f\"\\n{name} DataLoader:\")\n",
        "    print(f\"Number of batches: {len(loader)}\")\n",
        "    print(f\"Total samples: {len(loader.dataset)}\")\n",
        "    for batch in loader:\n",
        "        print(\"Sample batch shapes:\")\n",
        "        print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
        "        print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
        "        print(f\"  labels: {batch['labels'].shape}\")\n",
        "        sample_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
        "        sample_label = batch['labels'][0].item()\n",
        "        print(f\"Sample text: {sample_text}\")\n",
        "        print(f\"Sample label: {sample_label}\")\n",
        "        break\n",
        "\n",
        "verify_dataloader(train_loader, \"Training\")\n",
        "verify_dataloader(val_loader, \"Validation\")\n",
        "verify_dataloader(test_loader, \"Test\")"
      ],
      "metadata": {
        "id": "lVXmp1a1OvNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "tZMrFVQqP7tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define focal loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha  # Class weights\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss"
      ],
      "metadata": {
        "id": "-Mk9keTfQAp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6).to(device)"
      ],
      "metadata": {
        "id": "coBnVPQWQTtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "scaler = GradScaler()  # For mixed precision\n",
        "num_epochs = 3  # Adjustable\n",
        "best_val_loss = float('inf')\n",
        "checkpoint_dir = '/content/drive/MyDrive/checkpoints/'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "NOr7cRGgRaAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = FocalLoss(alpha=class_weights, gamma=2.0)(outputs.logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = FocalLoss(alpha=class_weights, gamma=2.0)(outputs.logits, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = f\"{checkpoint_dir}/epoch_{epoch+1}.pth\"\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pth\")\n",
        "        print(f\"Saved best model: {checkpoint_dir}/best_model.pth\")"
      ],
      "metadata": {
        "id": "3CBzT8VHRjkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#evaluation\n"
      ],
      "metadata": {
        "id": "wWLrrYIxRvhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test evaluation\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        test_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NKFcpHNtRx-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
        "plt.title(\"Test Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VMf7n-UlR0kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#saving"
      ],
      "metadata": {
        "id": "PPVfeKuQXqKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and save confusion matrix\n",
        "try:\n",
        "    cm = confusion_matrix(test_labels, test_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
        "    plt.title(\"Test Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    # Ensure directory exists and save with single slash\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    plt.savefig(f\"{checkpoint_dir}/confusion_matrix.png\", dpi=300)\n",
        "    print(f\"Saved confusion matrix to {checkpoint_dir}/confusion_matrix.png\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error generating confusion matrix: {e}\")\n",
        "\n",
        "# Save model and results\n",
        "torch.save(model.state_dict(), f\"{checkpoint_dir}/final_model.pth\")\n",
        "print(f\"Saved final model to {checkpoint_dir}/final_model.pth\")\n",
        "\n",
        "# Save training history\n",
        "training_history = {\n",
        "    'epoch': list(range(1, num_epochs + 1)),\n",
        "    'train_loss': [0.1066, 0.0616, 0.0433],  # From your output\n",
        "    'val_loss': [0.5346, 0.5891, 0.6356],\n",
        "    'val_accuracy': [0.7308, 0.7359, 0.7332]\n",
        "}\n",
        "history_df = pd.DataFrame(training_history)\n",
        "history_df.to_csv(f\"{checkpoint_dir}/training_history.csv\", index=False)\n",
        "print(f\"Saved training history to {checkpoint_dir}/training_history.csv\")\n",
        "\n",
        "# Download files to your PC (for VSCode)\n",
        "from google.colab import files\n",
        "try:\n",
        "    files.download(f\"{checkpoint_dir}/final_model.pth\")\n",
        "    #files.download(f\"{checkpoint_dir}/confusion_matrix.png\")\n",
        "    #files.download(f\"{checkpoint_dir}/training_history.csv\")\n",
        "    print(\"Files downloaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Download error: {e}. Attempting to zip and download checkpoints directory.\")\n",
        "    !zip -r checkpoints.zip /content/drive/MyDrive/checkpoints/\n",
        "    files.download('checkpoints.zip')\n",
        "    print(\"Downloaded checkpoints.zip containing all files.\")"
      ],
      "metadata": {
        "id": "QzOt4vIKYUBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}